{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "#This code imports various libraries and classes necessary for data analysis and machine learning using Apache Spark.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, count, desc, explode, lit, min, max, split, stddev, udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves an existing SparkSession or creates a new one if it doesn't exist. This ensures that only one SparkSession is active per JVM (Java Virtual Machine).\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Capstone Project\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reads the JSON data from the specified file into a DataFrame.\n",
    "sparkify_data = 'mini_sparkify_event_data.json'\n",
    "df = spark.read.format(\"json\").load(sparkify_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#It is used to show the structure of the DataFrame in the form of a tree\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#used to convert the Apache Spark df DataFrame to a Pandas DataFrame\n",
    "df1 = df.toPandas()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to get information about the DataFrame\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code extracts the unique values from various columns of the df1 DataFrame and displays them to the console, allowing us to better understand the different values ​​that appear in those columns and gain insight into the diversity of data in those categories.\n",
    "unique_levels = df1['level'].unique()\n",
    "unique_methods = df1['method'].unique()\n",
    "unique_pages = df1['page'].unique()\n",
    "unique_statuses = df1['status'].unique()\n",
    "unique_genders = df1['gender'].unique()\n",
    "unique_auths = df1['auth'].unique()\n",
    "\n",
    "print(unique_levels, unique_methods, unique_pages, unique_statuses, unique_genders, unique_auths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code counts the number of records in the df1 DataFrame that have an empty value in the 'userId' column and displays that count to the console. This allows us to identify how many records in the dataset do not have a 'userId' assigned to them and gives us information about the integrity of the data in that column.\n",
    "empty_user_ids_count = df1.loc[df1['userId'] == '', 'userId'].count()\n",
    "print(empty_user_ids_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code counts the number of records in the DataFrame df that have an empty string as value in the 'sessionId' column. This allows us to identify how many records have an empty 'sessionId' in the dataset and gives us information about the integrity of the data in that column.\n",
    "df.filter(df.sessionId == '').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code first filters the DataFrame df to remove rows that have an empty 'userId' and then counts the number of records that still have an empty 'userId'. This allows us to check how many records have been deleted and gives us information about the integrity of the data in the 'userId' column.\n",
    "count = df.filter((df.userId != '') & (df.userId.isNull() | (df.userId == ''))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the Spark DataFrame df to a Pandas DataFrame called df1\n",
    "df1 = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeks to get the unique values of various columns of the dfp DataFrame and store them in separate variables. Unique values ​​are obtained using the set() function, which removes duplicates, and then converted to lists using list(), this allows you to check what unique values ​​exist in each column and can be useful for better understanding the distribution and diversity of the data in those columns.\n",
    "unique_levels = list(set(df1['level']))\n",
    "unique_methods = list(set(df1['method']))\n",
    "unique_pages = list(set(df1['page']))\n",
    "unique_statuses = list(set(df1['status']))\n",
    "unique_genders = list(set(df1['gender']))\n",
    "unique_auths = list(set(df1['auth']))\n",
    "\n",
    "print(unique_levels, unique_methods, unique_pages, unique_statuses, unique_genders, unique_auths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows and columns\n",
    "row_count = df.count()\n",
    "column_count = len(df.columns)\n",
    "\n",
    "# print the count\n",
    "print((row_count, column_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get basic information about the 'userId' column, such as the number of non-null values, the variability of the values, and other important statistics. It can be useful to better understand the distribution and properties of the 'userId' column in the dataset.\n",
    "summary_df = df.select('userId').describe()\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Get basic information about the 'sessionId' column, such as the number of non-null values, the variability of the values, and other important statistics. It can be useful to better understand the distribution and properties of the 'sessionId' column in the dataset.\n",
    "summary_df = df.select('sessionId').summary()\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get an overview of the most frequent and least frequent pages visited by users in the data set. It can help identify usage patterns and understand what actions are most common among users.\n",
    "summary_df = df.groupBy('page').count()\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#identify the different categories or types of pages present in the data set. It can help to understand the variety of actions or events logged and provide an initial idea of ​​the available features\n",
    "unique_pages_df = df.select('page').distinct()\n",
    "unique_pages_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get an overview of the 'length' column, which can help you understand the length characteristics of recorded songs or events.\n",
    "summary_df = df.select('length').summary()\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#convert Spark's df DataFrame to a Pandas DataFrame\n",
    "df_pandas = df.toPandas()\n",
    "print(df_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a list called unique_counts that contains information about the number of unique values in the 'userId', 'page' and 'sessionId' columns of the DataFrame\n",
    "unique_counts = [f'Unique {col}s: {df.select(col).distinct().count()}' for col in ('userId', 'page', 'sessionId')]\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the number of unique values in the 'userId' column of the DataFrame df and stores it in the unique_user_ids_count variable\n",
    "unique_user_ids_count = len(df.select('userId').distinct().collect())\n",
    "print(unique_user_ids_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code assigns values of 1 to the \"churn\" column for the records that represent a cancellation confirmation, and then calculates the cumulative sum of these values ​​within a window defined by the \"userId\" partition. This can help identify when a user has canceled based on the cumulative evolution of the churn records for each user.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "windowval = Window.partitionBy(\"userId\").rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "df = df.withColumn(\"churn\", F.expr(\"CASE WHEN page = 'Cancellation Confirmation' THEN 1 ELSE 0 END\"))\n",
    "df = df.withColumn(\"churn\", F.sum(\"churn\").over(windowval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays the user count for each unique 'churn' value, providing information about the distribution of churns and non-churns in the dataset\n",
    "churn_counts = df1.drop_duplicates(subset='userId').groupby('churn')['userId'].count()\n",
    "print(churn_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#performs a grouping and aggregation of data in the DataFrame df using the 'churn' column as the grouping criteria\n",
    "df.groupby('churn').agg(F.countDistinct('userId').alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs a grouping and aggregation of data in the DataFrame df using the 'churn' and 'gender' columns as grouping criteria.\n",
    "df.groupby('churn', 'gender').agg(F.countDistinct('userId').alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs a grouping and aggregation of data in the DataFrame df using the 'churn' and 'level' columns as grouping criteria\n",
    "df.groupby('churn', 'level').agg(F.countDistinct('userId').alias('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate a bar chart showing the frequency of unique users in different categories\n",
    "def plot_frequency(subset, group, labels, x_title=\"Subscription\", y_title=\"users\"):\n",
    "    freq_counts = df1.drop_duplicates(subset=subset).groupby(group)['userId'].count()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    freq_counts.plot(kind='bar', ax=ax, color=['red', 'black'])\n",
    "    ax.set_title('Unique users per category')\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_xticklabels(labels, rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "plot_frequency(['userId'], ['churn'], ['Active', 'Cancelled'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status of \"cancelled\" accumulates 32% of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view unique user count by gender and subscription status\n",
    "import numpy as np\n",
    "def plot_frequency(subset, group, labels, x_title=\"Subscription\", y_title=\"users\"):\n",
    "    freq_counts = df1.drop_duplicates(subset=subset).groupby(group)['userId'].count()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    colors=['yellow','green','blue','gray']\n",
    "    ax.bar(x, freq_counts, color=colors)\n",
    "    ax.set_title('unique users per category')\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "plot_frequency(['userId', 'gender'],\n",
    "               ['gender', 'churn'],\n",
    "               ['Active-Female', 'Cancelled-Female', 'Active-Male', 'Cancelled-Male'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a direct relationship between the highest number of active subscriptions and the highest number of canceled subscriptions that, taking into account the gender, is male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#shows the frequency of unique users in different categories\n",
    "def plot_frequency(subset, group, labels, x_title=\"Subscription\", y_title=\"users\"):\n",
    "    freq_counts = df1.drop_duplicates(subset=subset).groupby(group)['userId'].count()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    x = np.arange(len(labels))\n",
    "    \n",
    "    colors=['pink','orange','brown','purple']\n",
    "    ax.bar(x, freq_counts, color=colors)\n",
    "    ax.set_title('Unique users per category')\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "plot_frequency(['userId', 'level'],\n",
    "               ['level', 'churn'],\n",
    "               ['Active-Free', 'Cancelled-Free', 'Active-Paid', 'Cancelled-Paid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85% of total active Sparkify users are paid subscriptions, 77% of total canceled users were paid subscriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The provided code performs an analysis of the event pages based on the cancellation status of the users. First, the percentage occurrence of events for canceled and active users on each page is calculated. A bar chart is then created showing the comparison of these percentages between the two groups of users. The graph provides a visualization of the difference in event distribution between canceled users and active users on different pages.\n",
    "users_1 = df1[df1.churn == 1].groupby(['page'])['userId'].count().drop('NextSong')\n",
    "users_1 = users_1 / users_1.sum() * 100\n",
    "\n",
    "users_0 = df1[df1.churn == 0].groupby(['page'])['userId'].count().drop('NextSong')\n",
    "users_0 = users_0 / users_0.sum() * 100\n",
    "\n",
    "users_df = pd.DataFrame({'Cancelled': users_1, 'Active users': users_0})\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "users_df.plot(kind='bar', ax=ax, color=['red', 'black'])\n",
    "ax.set_xlabel('Event occurrence (%)')\n",
    "ax.set_ylabel('Page')\n",
    "ax.set_title('Event occurrence for active and cancelled users')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activity on the page: Add friend and Add to playlist are notable in relation to the use of Sparkify, the activity with the highest use of the page: Thumps up is for both active users and those who have cancelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform an operation on the DataFrame 'df' to get a new DataFrame called 'churn_users'. This new DataFrame contains the 'userId' and 'churn' columns and is obtained by removing duplicate rows based on these two columns. Then, the content of the 'churn_users' DataFrame is displayed in the output. This allows us to see the unique values ​​of 'userId' along with their 'churn' status.\n",
    "\n",
    "churn_users = df.drop_duplicates(['userId', 'churn']).select('userId', 'churn')\n",
    "churn_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The provided code defines a function called 'create_dummy_df' which creates a new DataFrame from another existing DataFrame. In this case, it is used to create a DataFrame named 'gender_df' containing the 'userId' and 'gender' columns. The function replaces the values ​​in the 'gender' column based on a given dictionary, converts the data type of the 'gender' column to an integer, and displays the schema and content of the resulting new DataFrame. In this case, 'M' values ​​are replaced with '1' and 'F' values ​​are replaced with '0'. This approach allows converting categorical values ​​into numerical variables for further analysis or modeling.\n",
    "def create_supposed_df(col, dictionary):\n",
    "   \n",
    "    col_df = df.select('userId', col).dropDuplicates().replace(dictionary, subset=col)\n",
    "    col_df = col_df.withColumn(col, col_df[col].cast('int'))\n",
    "    col_df.printSchema()\n",
    "    col_df.show()\n",
    "    return col_df\n",
    "\n",
    "gender_df = create_supposed_df('gender', {'M': '1', 'F': '0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The function would use an existing DataFrame and replace the values in the 'level' column based on a given dictionary. In this case, 'paid' values ​​would be replaced with '1' and 'free' values ​​would be replaced with '0'. Then the resulting DataFrame would display its schema and content. This approach allows you to convert the values ​​of the 'level' column into numeric variables for further analysis or modeling.\n",
    "level_df = create_supposed_df('level', {'paid': '1', 'free': '0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#number of different artists present in the 'artist' column of the DataFrame 'dfp'\n",
    "df1.artist.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The code performs some data processing to calculate the average length and standard deviation of the songs played by each user\n",
    "from pyspark.sql import functions as sF\n",
    "\n",
    "song_length = df.filter(df.page == 'NextSong') \\\n",
    "    .select('userId', 'sessionId', 'length') \\\n",
    "    .withColumn('hours', sF.expr('length / 3600')) \\\n",
    "    .groupBy('userId', 'sessionId') \\\n",
    "    .agg(sF.sum('hours').alias('total_hours'))\n",
    "\n",
    "song_length = song_length.groupBy('userId') \\\n",
    "    .agg(\n",
    "        sF.avg('total_hours').alias('mean_hours'),\n",
    "        sF.stddev('total_hours').alias('stdev_hours')\n",
    "    ).fillna(0)\n",
    "\n",
    "song_length.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform data processing to obtain the distribution of the pages visited by each user in the form of normalized percentages. This is achieved by creating a new DataFrame 'user_page_distribution' containing the 'userId' columns and the pages columns, where each value represents the percentage of visits to that page per user\n",
    "from functools import reduce\n",
    "\n",
    "user_page_distribution = df.groupby('userId').pivot('page').count().na.fill(0)\n",
    "user_page_distribution = user_page_distribution.drop(*['Cancel','Cancellation Confirmation'])\n",
    "pages_cols = user_page_distribution.columns[1:]\n",
    "new_df = user_page_distribution.withColumn('total', sum(user_page_distribution[col] for col in pages_cols))\n",
    "for col in pages_cols:\n",
    "    new_df = new_df.withColumn(f'norm_{col}', new_df[col] / new_df['total'] * 100.)\n",
    "new_df = new_df.drop('total')\n",
    "new_df = new_df.drop(*pages_cols)\n",
    "oldColumns = new_df.columns\n",
    "newColumns = ['userId'] + pages_cols\n",
    "user_page_distribution = reduce(lambda new_df, idx: new_df.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), new_df)\n",
    "new_df=None\n",
    "\n",
    "user_page_distribution.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the average and standard deviation of the number of songs played per session for each user\n",
    "\n",
    "song_user_df = (df.filter(df.page == 'NextSong')\n",
    "                  .groupBy('userId', 'sessionId')\n",
    "                  .agg(F.count('*').alias('count'))\n",
    "                  .groupBy('userId')\n",
    "                  .agg(F.avg('count').alias('mean_songs'),\n",
    "                       F.stddev('count').alias('stdev_songs'))\n",
    "                  .na.fill(0))\n",
    "\n",
    "song_user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the number of unique artists listened to by each user\n",
    "artists_user_fans = (\n",
    "    df.select('userId', 'artist')\n",
    "    .dropDuplicates()\n",
    "    .groupBy('userId')\n",
    "    .count()\n",
    "    .withColumnRenamed('count', 'num_artists')\n",
    ")\n",
    "\n",
    "artists_user_fans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the duration of each user session in hours from the recorded timestamps\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "session_end = (\n",
    "    df.groupBy('userId', 'sessionId')\n",
    "    .agg(expr('max(ts)').alias('end'))\n",
    ")\n",
    "\n",
    "session_start = (\n",
    "    df.groupBy('userId', 'sessionId')\n",
    "    .agg(expr('min(ts)').alias('start'))\n",
    ")\n",
    "\n",
    "ticks_per_hour = 1000 * 60 * 60\n",
    "\n",
    "session_df = (\n",
    "    session_start.join(session_end, ['userId', 'sessionId'])\n",
    "    .withColumn('session_hours', (col('end') - col('start')) / ticks_per_hour)\n",
    "    .select('userId', 'sessionId', 'session_hours')\n",
    ")\n",
    "\n",
    "session_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the mean and standard deviation of session durations per user\n",
    "from pyspark.sql.functions import avg, stddev, col\n",
    "\n",
    "session_user_df = (\n",
    "    session_df.groupBy('userId')\n",
    "    .agg(avg('session_hours').alias('mean_session_h'), stddev('session_hours').alias('stdev_session_h'))\n",
    "    .na.fill(0)\n",
    ")\n",
    "\n",
    "session_user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the number of sessions for each user, eliminating duplicate sessions\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "num_sessions_user_df = (\n",
    "    df.select('userId', 'sessionId')\n",
    "    .dropDuplicates()\n",
    "    .groupby('userId')\n",
    "    .agg(count('*').alias('num_sessions'))\n",
    ")\n",
    "\n",
    "num_sessions_user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the number of days since subscription for each user\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "def days_since_subscription(df, col_name='days_on'):\n",
    "    reg_df = df.select('userId', 'registration') \\\n",
    "        .dropDuplicates() \\\n",
    "        .withColumnRenamed('registration', 'start') \\\n",
    "        .join(df.groupBy('userId').max('ts').withColumnRenamed('max(ts)', 'end'), 'userId') \\\n",
    "        .withColumn(col_name, F.expr('(end - start) / (1000 * 60 * 60 * 24)')) \\\n",
    "        .select('userId', col_name)\n",
    "    \n",
    "    return reg_df\n",
    "\n",
    "reg_df = days_since_subscription(df, col_name='days_total_subscription')\n",
    "reg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters the original DataFrame to get only the rows with 'paid' subscription level, then calculates the number of days since paid subscription for each user\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_paid = df.filter(expr(\"level = 'paid'\"))\n",
    "paid_df = days_since_subscription(df_paid, col_name='days_paid_subscription')\n",
    "paid_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code filters and processes two different subsets of the original data frame df based on the level (\"free\" and \"paid\"). Then calculate the subscription duration in days for each user in each subset\n",
    "df_free = df.filter(expr(\"level = 'free'\"))\n",
    "free_df = days_since_subscription(df_free, col_name='days_free_subscription')\n",
    "free_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both lists contain a collection of dataframes related to various user characteristics. The names in user_features_names provided a reference to identify each data frame in user_features.\n",
    "user_features = []\n",
    "user_features_names = []\n",
    "\n",
    "user_features.append(gender_df)\n",
    "user_features_names.append('gender_df')\n",
    "\n",
    "user_features.append(song_length)\n",
    "user_features_names.append('song_length')\n",
    "\n",
    "user_features.append(user_page_distribution)\n",
    "user_features_names.append('user_page_distribution')\n",
    "\n",
    "user_features.append(song_user_df)\n",
    "user_features_names.append('song_user_df')\n",
    "\n",
    "user_features.append(artists_user_fans)\n",
    "user_features_names.append('artists_user_fans')\n",
    "\n",
    "user_features.append(session_user_df)\n",
    "user_features_names.append('session_user_df')\n",
    "\n",
    "user_features.append(num_sessions_user_df)\n",
    "user_features_names.append('num_sessions_user_df')\n",
    "\n",
    "user_features.append(reg_df)\n",
    "user_features_names.append('reg_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs the union of multiple dataframes of user characteristics with a base dataframe, generating a final dataframe with all the characteristics combined. Each feature dataframe is joined using the 'userId' column and the result is sorted by 'userId'\n",
    "\n",
    "final_df = churn_users\n",
    "\n",
    "def join_features(base, new):\n",
    "    return base.join(new, 'userId', how='inner').dropDuplicates()\n",
    "\n",
    "for feature, feature_name in zip(user_features, user_features_names):\n",
    "    final_df = join_features(final_df, feature)\n",
    "\n",
    "final_df = final_df.orderBy('userId', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code deletes a directory if it already exists and then saves the final_df dataframe in CSV format\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "directory_path = 'user_dataset.CSV'\n",
    "\n",
    "if os.path.exists(directory_path):\n",
    "    shutil.rmtree(directory_path)\n",
    "\n",
    "final_df.write.save('user_dataset.CSV', format='csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the Spark DataFrame to a Pandas DataFrame\n",
    "final_df1 = final_df.toPandas()\n",
    "final_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1.to_csv('user_dataset_definitive.CSV', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads the CSV file into a Spark DataFrame, converts it to a Pandas DataFrame\n",
    "final_df = spark.read.csv('user_dataset.CSV', header = True)\n",
    "final_1_df = final_df.toPandas()\n",
    "final_1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renames the columns of the final_df DataFrame, replacing whitespace with underscores and removing periods in the column names, and then displays the new column names\n",
    "final_df = final_df.toDF(*(col.replace(' ', '_').replace('.', '') for col in final_df.columns))\n",
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the 'userId' columns and the first 11 columns of the final_df DataFrame to the IntegerType data type, and the remaining columns (starting with column 12) are converted to the FloatType data type. This is achieved using Spark SQL's withColumn() and cast() function\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "final_df = final_df.select(\n",
    "    col('userId').cast(IntegerType()),\n",
    "    *[\n",
    "        col(col_name).cast(IntegerType()) if col_name in final_df.columns[1:12]\n",
    "        else col(col_name).cast(FloatType())\n",
    "        for col_name in final_df.columns[1:]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace null values (NaN) with the value 0\n",
    "final_df = final_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepares the data for use in a machine learning model by assembling the features, scaling them, and dividing them into training and test sets, 70% of the data is assigned to the training set (train) and 30% of the data to the test set. A seed of 100 is used to guarantee the reproducibility of the division\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=final_df.columns[2:], outputCol='features')\n",
    "assembled_data = assembler.transform(final_df)\n",
    "\n",
    "scaler = StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='scaled_features')\n",
    "scaled_data = scaler.fit(assembled_data).transform(assembled_data)\n",
    "\n",
    "ml_data = scaled_data.select(scaled_data.churn.alias('label'), scaled_data.scaled_features.alias('features'))\n",
    "\n",
    "train, test = ml_data.randomSplit([0.70, 0.30], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs fit of a machine learning model using cross validation and saves the best fitted model to a specified file\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from datetime import datetime\n",
    "\n",
    "def model_fitting(data, model_type, param_grid, save_as, num_folds=2, random_seed=100):\n",
    "    model_evaluator = CrossValidator(\n",
    "        estimator=model_type,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=MulticlassClassificationEvaluator(),\n",
    "        numFolds=num_folds,\n",
    "        seed=random_seed\n",
    "    )\n",
    "    t_start = datetime.now()\n",
    "    fitted_model = model_evaluator.fit(data)\n",
    "    t_dif = datetime.now() - t_start\n",
    "    t_start = datetime.now()\n",
    "    fitted_model.bestModel.write().overwrite().save(save_as)\n",
    "    t_dif = datetime.now() - t_start\n",
    "    return fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(model.regParam, [0.02, 0.2])\n",
    "              .addGrid(model.elasticNetParam, [0.1, 0.4])\n",
    "              .addGrid(model.aggregationDepth, [3, 6])\n",
    "              .build())\n",
    "\n",
    "m = model_fitting(train, model, param_grid, 'LogisticRegression.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.transform(test)\n",
    "\n",
    "print(\"test\")\n",
    "# Calculate and display the precision of the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Calculate and display the accuracy metric\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "# Calculate and display the recall metric\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "# Calculate and display the F1-score metric\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.transform(train)\n",
    "\n",
    "print(\"train\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trains a decision tree classifier with different combinations of hyperparameters, saves the trained model, and then returns the classifier-specific evaluation metrics for the training and test sets\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(model.maxDepth, [1, 3, 5])\n",
    "              .addGrid(model.impurity, ['entropy', 'gini'])\n",
    "              .build())\n",
    "\n",
    "m = model_fitting(train, model, param_grid, 'DecisionTreeClassifier.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.transform(test)\n",
    "\n",
    "print(\"test\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.transform(train)\n",
    "\n",
    "print(\"train\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient-Boosted Trees (GBTs) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains a Gradient Boosted Trees (GBT) classifier with different combinations of hyperparameters, saves the trained model, and displays classifier-specific evaluation metrics for the training and test sets\n",
    "\n",
    "model = GBTClassifier()\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(model.maxDepth, [1, 3, 5]) \\\n",
    "    .addGrid(model.maxBins, [4, 2]) \\\n",
    "    .addGrid(model.maxIter, [6, 2]) \\\n",
    "    .build()\n",
    "\n",
    "m = model_fitting(train, model, param_grid, 'GradientBoostedTrees.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.transform(test)\n",
    "\n",
    "print(\"test\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.transform(train)\n",
    "\n",
    "print(\"train\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finds the best combination of hyperparameters for the RandomForestClassifier classifier using the cross-validation technique with the default number of folds \n",
    "\n",
    "model = RandomForestClassifier()\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(model.maxDepth, [1, 3]) \\\n",
    "    .addGrid(model.impurity, ['entropy', 'gini']) \\\n",
    "    .addGrid(model.maxBins, [4, 2]) \\\n",
    "    .addGrid(model.numTrees, [6, 2]) \\\n",
    "    .addGrid(model.featureSubsetStrategy, ['sqrt', 'onethird']) \\\n",
    "    .build()\n",
    "\n",
    "m = model_fitting(train, model, param_grid, 'RandomForestClassifier.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.transform(test)\n",
    "\n",
    "print(\"test\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = m.transform(train)\n",
    "\n",
    "print(\"train\")\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
